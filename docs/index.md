<font color="#ffffff" > Synthax - An Experiment </font>

Synthax is an effort to explore the less popular and unknown optimizer algorithm used for optimization of neural networks. This whole experiment revolves around comparing two types of optimizers: Gradient Descent and Newton Second Moment Update.

This project leverages JAX for stable autodiff and Sympy for Symbolical notation of various Mathematical equations.

![Synthax Logo](assets/logo/synthax.png){: width="800" height="1200" style="opacity: 0.8; background-color: #ffffff;"}

- [Introduction](Introduction.md)
- [Source Code](SourceCode.md)
- [Process](Process.md)
- [Conclusion](Conclusion.md)

For more information, visit [GitHub](https://github.com/udit-rawat/Synthax).
